{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b4502e",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/r-doz/PML2025/blob/main/./07_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 7: Markov Chain Monte Carlo\n",
    "\n",
    "This notebook is a revisitation of Ginevra Carbone's work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you wish to draw samples from a posterior distribution $p(\\theta|x)$ and that the computation of the normalization factor \n",
    "\n",
    "$$p(x) = \\int_\\theta p(x|\\theta) p (\\theta) d \\theta$$ \n",
    "\n",
    "is intractable, due to the high dimensionality of the problem. \n",
    "\n",
    "MCMC is a class of algorithms that allow to perform sampling from the **unkown probability distribution** $p(\\theta|x)$,  whose density is proportional to the **known unnormalized factor** $p(x|\\theta) p (\\theta)$.\n",
    "\n",
    "The basic idea is that of building a Markov chain that has the desired distribution as its equilibrium distribution. This way, one can obtain a sample of the desired distribution by recording states from the chain and the more steps that are included, the more closely the distribution of the sample matches the desired distribution $p(\\theta|x)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis Hastings from scratch\n",
    "\n",
    "Assume that we want to sample from a multimodal distribution of which we know only the unnormalized distribution, which is a 1-D Gaussian Mixture. For example, consider the unnormalized probability density function $\\tilde{p}(x)$ defined as a mixture of three Gaussian distributions:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(x) = \\mathcal{N}(x; 0, 1) + \\mathcal{N}(x; 5, 3) + \\mathcal{N}(x; 10, 2)\n",
    "$$\n",
    "\n",
    "This distribution is not normalized, and our goal is to sample from it using the Metropolis-Hastings algorithm.\n",
    "\n",
    "We use a proposal distribution $q(y \\mid x)$ defined as a Gaussian centered at the current state $x$ with standard deviation $\\sigma$:\n",
    "\n",
    "$$\n",
    "q(y \\mid x) = \\mathcal{N}(y; x, \\sigma)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.mcmc import MCMC, HMC, NUTS\n",
    "import torch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_q(y, sigma):\n",
    "    #your code here\n",
    "\n",
    "def q_y_given_x(y, x, sigma):\n",
    "    #your code here\n",
    "\n",
    "def p_tilde(x):\n",
    "    #your code here\n",
    "\n",
    "def metropolis_hastings(n, sigma):\n",
    "    rejections = 0\n",
    "    x = np.zeros(n)\n",
    "    x[0] = np.random.normal(0, 1)\n",
    "    for i in range(1, n):\n",
    "        #your code here\n",
    "    return x, rejections/n\n",
    "\n",
    "\n",
    "#In this case, we can consider the gaussian mixture where each component has the same weight\n",
    "def p(x):\n",
    "    return (stats.norm(0, 1).pdf(x) + stats.norm(5, 3).pdf(x) + stats.norm(10, 2).pdf(x))/3\n",
    "\n",
    "t = np.linspace(-5, 20, 1000)\n",
    "plt.plot(t, p_tilde(t), label='p_tilde')\n",
    "plt.plot(t, p(t), label='p')\n",
    "plt.legend()\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to consider also a burn-in phase, monitor the rejection rate and plot the rejection rate as a function of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "burn_in = 1000 # number of samples to discard before collecting data\n",
    "n = 5000\n",
    "x,r = metropolis_hastings(n, sigma)\n",
    "x = x[burn_in:]\n",
    "print('Rejection rate:', r)\n",
    "plt.plot(t, p(t), label='p(x)')\n",
    "plt.hist(x, bins=50, density=True, label='samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = []\n",
    "sigma_range = np.linspace(0.01, 2, 10)\n",
    "for sigma_i in sigma_range:\n",
    "    x,r_i = metropolis_hastings(5000, sigma_i)\n",
    "    r.append(r_i)\n",
    "\n",
    "plt.plot(sigma_range, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditioning in Pyro\n",
    "\n",
    "Probabilistic programming allows us to condition generative models on observed data and to perform inference on it. In Pyro, we can separate the conditioning step from its evaluation via inference: first write a model and then condition it on many different observations.\n",
    "\n",
    "We are trying to measure the weight of an object, but we don't have access to the true weights, because the scale is unreliable. \n",
    "Instead, we can use a **model of the measurement noise** plus an initial **uncertain guess** (based on some properties of the object) to obtain an **uncertain estimate of the true weight**. \n",
    "\n",
    "$$weight \\, | \\, guess \\sim \\mathcal{N}(guess, 1)$$\n",
    "\n",
    "$$ measurement \\, | \\, guess, weight \\sim \\mathcal{N}(weight, 0.75^2) $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(guess=8.5):\n",
    "    \n",
    "    # models weight in terms of the initial guess\n",
    "    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.0))\n",
    "\n",
    "    # models measurement noise\n",
    "    measurement = pyro.sample(\"measurement\", dist.Normal(weight, 0.75))\n",
    "    \n",
    "    return measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we observe a measurement of the object corresponding to 9.5 kg. We want to sample from the distribution of the weight given both the observation `measurement = 9.5` and an input knowledge `guess = 8.5`. In other words, we wish to infer the distribution \n",
    "\n",
    "$$weight \\, | \\, guess, measurement=9.5 \\sim ?$$\n",
    "\n",
    "Pyro provides a method called `pyro.condition` that takes as inputs a model and a dictionary of observations and returns a new model which is conditioned on the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition the model on a single observation\n",
    "obs = torch.tensor(9.5)\n",
    "single_conditioned_scale = pyro.condition(scale, data={\"measurement\": obs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`conditioned_scale()` could be equivalently defined by using the `obs` parameter directly inside the model definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditioned_scale(obs, guess=8.5): \n",
    "    weight = pyro.sample(\"weight\", dist.Normal(guess, 1.))\n",
    "    measurement = pyro.sample(\"measurement\", dist.Normal(weight, 1.), obs=obs)\n",
    "    return measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian Monte Carlo\n",
    "\n",
    "Hamiltonian Monte Carlo (HMC) combines an approximate Hamiltonian dynamics simulation and a Metropolis-Hastings acceptance step. It is designed to reduce the problems of *low acceptance rates* and *autocorrelation* bewteen consecutive samples.\n",
    "\n",
    "HMC introduces an additional *momentum* variable with the aim of favouring the exploration of the typical set, instead of drifting towards the tails or the mode of $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMC on conditioned scale model\n",
    "\n",
    "Let's perform inference on the conditioned scale model by using HMC and a few `measurement` observations, with the aim of getting an estimate of the expected weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One chain of MCMC\n",
    "hmc_kernel = HMC(model=conditioned_scale) # transition kernel\n",
    "mcmc = MCMC(hmc_kernel, num_samples=500, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`warmup_steps` refers to the **burn-in** period, which is the number of discarded samples before performing the actual sampling of `num_samples` values from the (hopefully) stationary distribution.\n",
    "\n",
    "`num_chains` is the number of independent MCMC runs, subject to different initializations. Pyro automatically computes them in parallel.\n",
    "\n",
    "`mcmc.summary()` prints a summary table displaying the diagnostics of posterior samples. In particular, it includes:\n",
    "- $5\\%$ and $95\\%$ credibility intervals for the estimates;\n",
    "- the number of effective samples `n_eff`, representing the number of independent draws from the posterior distribution;\n",
    "- the split-$\\hat{R}$ statistic `r_hat`, which monitors whether the chains have converged to the equilibrium distribution or not. \n",
    "\n",
    "Values above 1 for `r_hat` and low values for `n_eff` indicate that some chains have not fully converged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement observations\n",
    "#obs = torch.tensor([8.2, 9.6, 7.8, 9.1])\n",
    "\n",
    "# suppose the true value is 9:\n",
    "obs = torch.normal(9.0, 0.75, (20,))\n",
    "print(obs)\n",
    "\n",
    "# posteriors \n",
    "posterior = mcmc.run(obs=obs)\n",
    "\n",
    "# dictionary of sampled values\n",
    "mcmc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc_samples = mcmc.get_samples()\n",
    "chain = mcmc_samples[\"weight\"]\n",
    "n_samples = chain.shape[0]\n",
    "print(chain.shape)\n",
    "\n",
    "\n",
    "sns.lineplot(x=range(n_samples), y=chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"expected weight =\", mcmc_samples['weight'].mean().item())\n",
    "\n",
    "sns.histplot(mcmc_samples['weight'])\n",
    "plt.title(\"P( weight | measurement=obs )\")\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods_07 import conditioned_scale_file\n",
    "# multiple chains\n",
    "hmc_kernel = HMC(model=conditioned_scale_file) \n",
    "mcmc = MCMC(hmc_kernel, num_samples=500, warmup_steps=100, num_chains=3)\n",
    "\n",
    "posterior = mcmc.run(obs=obs)\n",
    "\n",
    "# extracting weight samples and grouping them by chains\n",
    "mcmc_samples = mcmc.get_samples(group_by_chain=True)\n",
    "chains = mcmc_samples[\"weight\"]\n",
    "\n",
    "n_chains, n_samples = chains.shape\n",
    "print(chains.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, n_chains, figsize=(12,3))\n",
    "for i, chain in enumerate(chains):\n",
    "    sns.lineplot(x=range(n_samples), y=chain, ax=ax[i])\n",
    "    ax[i].set_title(\"chain \"+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"expected weight =\", mcmc_samples['weight'].mean().item())\n",
    "fig, ax = plt.subplots(1, n_chains, figsize=(12,3))\n",
    "for i, chain in enumerate(chains):\n",
    "    sns.histplot(chain, ax=ax[i])\n",
    "    ax[i].set_title(\"chain \"+str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eight schools example\n",
    "\n",
    "Let's consider a hierarchical model of the Eight Schools dataset (Rubin, 1981), taken from chap. 5.5 of Gelman et al. \"Bayesian Data Analysis\" (2014). \n",
    "This study measures the effect of coaching programs on college admission tests and should reflect the knowledge that was acquired by the students from eight different schools.\n",
    "\n",
    "For each school ($j=1,\\ldots,8$) the dataset contains:\n",
    "- the estimated effect of coaching $y_j$\n",
    "- the standard error of the effect $\\sigma_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J = 8\n",
    "# estimated treatment effect\n",
    "y = torch.tensor([28.0, 8.0, -3.0, 7.0, -1.0, 1.0, 18.0, 12.0])\n",
    "# std of the estimated effect\n",
    "sigma = torch.tensor([15.0, 10.0, 16.0, 11.0, 9.0, 11.0, 10.0, 18.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some coaching programs have moderate effects (in the range 18–28 points), most have small\n",
    "effects (0–12 points), and two have small negative effects.\n",
    "\n",
    "We are interested in estimating the true effects of coaching $\\tau_j$, by using the Bayesian model\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu &\\sim \\mathcal{N}(0,5)\\\\\n",
    "\\tau &\\sim \\text{Half-Cauchy}(0,5)\\\\\n",
    "\\theta_j &\\sim \\mathcal{N}(\\mu,\\tau) \\;\\; j=1,\\ldots,8\\\\\n",
    "\\tau_j &\\sim \\mathcal{N}(\\theta_j,\\sigma_j) \\;\\;j=1,\\ldots,8\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods_07 import eight_school_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMC solves a two-state differential equation (over the parameters $\\theta$ and the momentum $\\rho$) using the *Leapfrog integrator* numerical integration algorithm. At each iteration, it takes $L$ discrete steps of some small time interval $\\epsilon$.\n",
    "\n",
    "We are now using **No U-Turn Sampler** (NUTS), a variant of HMC where $L$ and $\\epsilon$ are adaptively determined at each iteration, based on the curvature of the log-density: the step size $\\epsilon$ gets smaller in areas where the curvature is high, while the number of steps $L$ is large enough for the trajectory to move far through the posterior distribution without turning around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = NUTS(eight_school_file)\n",
    "mcmc = MCMC(kernel, warmup_steps=500, num_samples=100, num_chains=4)\n",
    "mcmc.run(J=J, sigma=sigma, y=y)\n",
    "mcmc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can notice some **divergent transitions**, arising when the simulated Hamiltonian trajectory departs from the true trajectory, potentially reducing the algorithm to a simple random walk. Consequently, if the number of divergent transitions is high, the estimates cannot be trusted.\n",
    "\n",
    "A **non-centered parametrization** for $\\theta_j$ defines the same statistical model, but makes inference more effective when the data is sparse. In fact, it is able to solve the pathologies we encountered in the centered model.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu &\\sim \\mathcal{N}(0,5)\\\\\n",
    "\\tau &\\sim \\text{Half-Cauchy}(0,5)\\\\\n",
    "\\nu_j &\\sim  \\mathcal{N}(0,1) \\;\\; j=1,\\ldots,8\\\\\n",
    "\\theta_j &= \\mu + \\tau\\, \\nu_j  \\;\\; j=1,\\ldots,8\\\\\n",
    "\\tau_j &\\sim \\mathcal{N} (\\theta_j,\\sigma_j) \\;\\;j=1,\\ldots,8\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods_07 import eight_schools_noncentered_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel = NUTS(eight_schools_noncentered_file)\n",
    "mcmc = MCMC(nuts_kernel, warmup_steps=500, num_samples=100, num_chains=4)\n",
    "mcmc.run(J=J, sigma=sigma, y=y)\n",
    "mcmc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- Bentacourt, \"A Conceptual Introduction to Hamiltonian Monte Carlo\", 2018\n",
    "- Bentacourt, Girolami, \"Hamiltonian Monte Carlo for Hierarchical Models\", 2013\n",
    "- Rubin D. B., \"Estimation in parallel randomized experiments\", 1981\n",
    "- Gelman et al., \"Bayesian Data Analysis\", 2014\n",
    "- [tutorial on deep probabilistic modeling](https://bookdown.org/robertness/causalml/docs/tutorial-on-deep-probabilitic-modeling-with-pyro.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
