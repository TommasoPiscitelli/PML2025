{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d877b276",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/r-doz/PML2025/blob/main/./Homeworks/Homework_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic Machine Learning -- Spring 2025, UniTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Marginal Likelihood Optimisation\n",
    "\n",
    "Given the Bayesian Linear Regression model implemented in the Notebook 6, considering the same dataset and variables (Apparent Temperature vs Humidity), optimize alpha and beta by maximizing the Marginal Likelihood.\n",
    "\n",
    "Note: You can show here only the piece of code that you used and write the optimal alpha and beta that you obtained (so you can run it in directly on Notebook 6)\n",
    "\n",
    "Hint: import scipy.optimize as optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Rejection Sampling\n",
    "\n",
    "Consider the unnormalized probability density function:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(x) = \\exp\\left(-\\frac{x^4}{4} - \\frac{x^2}{2}\\right)\n",
    "$$\n",
    "\n",
    "This density is defined for $x \\in \\mathbb{R}$, but it is not normalized. Let $Z$ be its normalization constant:\n",
    "\n",
    "$$\n",
    "Z = \\int_{-\\infty}^{\\infty} \\tilde{p}(x)\\, dx\n",
    "$$\n",
    "\n",
    "You are given access to a proposal distribution $q(x) = \\mathcal{N}(0, 1)$ from which you can sample and evaluate its density.\n",
    "\n",
    "-  Implement and then use rejection sampling with $q(x)$ to generate samples from the normalized target distribution $p(x) = \\frac{1}{Z} \\tilde{p}(x)$.\n",
    "    \n",
    "- Estimate the normalization constant $Z$ (hint: see your course notes!)\n",
    "\n",
    "- Compare your result with a numerical approximation of $Z$ using integration methods (e.g., scipy.integrate.quad).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sampling(target_pdf, proposal_pdf, proposal_sampler, n_samples):\n",
    "    samples = []\n",
    "    max_ratio = max(target_pdf(x) / proposal_pdf(x) for x in np.arange(-10, 10, 0.01)) # Adjust range as per your distribution\n",
    "    #print(f\"max_ratio: {max_ratio}\")\n",
    "    i =0 \n",
    "    \n",
    "    while len(samples) < n_samples:\n",
    "        x = proposal_sampler()  # Sample from proposal distribution\n",
    "        #print(f\"x numero: {i+1}\")\n",
    "        i = i + 1\n",
    "        u = np.random.uniform(0, max_ratio * proposal_pdf(x))\n",
    "        \n",
    "        if u <= target_pdf(x):\n",
    "            #print('entrato')\n",
    "            samples.append(x)\n",
    "        else:\n",
    "            print('non entrato')\n",
    "\n",
    "    print(i)\n",
    "    P_accepted = 1 - (i - len(samples)) / i\n",
    "    print(f\"Probability of being accepted: {P_accepted}\")\n",
    "    print(f\"partition function: {P_accepted * max_ratio}\")\n",
    "\n",
    "\n",
    "    return samples\n",
    "\n",
    "# Example usage:\n",
    "def target_pdf(x):\n",
    "    return np.exp((-x**4/4) - (x**2/2))    # Example target distribution, e.g., Gaussian\n",
    "def proposal_pdf(x):\n",
    "    return norm.pdf(x, loc=0, scale=1)    # Example proposal distribution, e.g., Exponential\n",
    "\n",
    "samples = rejection_sampling(target_pdf, proposal_pdf,  np.random.normal, n_samples=10000)\n",
    "\n",
    "Z, error = quad(target_pdf, -np.inf, np.inf)\n",
    "\n",
    "print(\"Valore dell'integrale Z:\", Z)\n",
    "print(\"Errore stimato:\", error)\n",
    "\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "plt.plot(x, target_pdf(x), label=\"target_pdf\")\n",
    "plt.plot(x, proposal_pdf(x), label=\"proposal_pdf\")\n",
    "#plt.plot(x, 18.521616940414205 * proposal_pdf(x), label=\"proposal_pdf (norm)\")\n",
    "_ = sns.histplot(samples, kde=True, stat='density', label='Normalized_target_pdf')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: MCMC Convergence Diagnostics\n",
    "\n",
    "You have implemented a Metropolis-Hastings algorithm and used it to draw samples from a univariate target distribution. In this exercise, you will compute convergence diagnostics from scratch to assess whether your chains have mixed well.\n",
    "\n",
    "Consider 4 parallel chains, each of length $N$ (after burn-in), starting from different initial values (use\n",
    "initial_values = [-10, -2, 2, 10]). The unnormalized probability density function $\\tilde{p}(x)$ is defined as a mixture of two Gaussian distributions:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(x) = \\mathcal{N}(x; -5, 1) + \\mathcal{N}(x; 5, 1)\n",
    "$$\n",
    "\n",
    "Perform the following steps two times, considering a proposal standard deviation of 0.1 and 2.0:\n",
    "\n",
    "- Compute the within variance $W$ and the between variance $B$\n",
    "- Compute the statistics $\\hat{R}$\n",
    "- For a single chain of samples $x_1, x_2, \\dots, x_N$, compute the lag-$k$ autocorrelation $\\rho_k$ and plot $\\rho_k$ for $k = 1, 2, \\dots, 20$\n",
    "- Estimate the effective number of samples $n_{eff}$\n",
    "\n",
    "Repeat the analysis with the distribution:\n",
    "\n",
    "$$\n",
    "\\tilde{p}(x) = \\mathcal{N}(x; -2, 1) + \\mathcal{N}(x; 2, 1)\n",
    "$$\n",
    "\n",
    "Discuss the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.mcmc import MCMC, HMC, NUTS\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics import tsaplots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_q(x, sigma):\n",
    "    #your code here\n",
    "    return np.random.normal(x, sigma)\n",
    "\n",
    "def q_y_given_x(y, x, sigma):\n",
    "    #your code here\n",
    "    return stats.norm(x, sigma).pdf(y)\n",
    "\n",
    "def p_tilde(x):\n",
    "    #your code here\n",
    "    return stats.norm(-2, 1).pdf(x) + stats.norm(2, 1).pdf(x) \n",
    "\n",
    "def metropolis_hastings(x_0, n, sigma):\n",
    "    rejections = 0\n",
    "    x = np.zeros(n)\n",
    "    x[0] = x_0\n",
    "    print(f\"Inizio da x[0] = {x[0]}\")\n",
    "    for i in range(1, n):\n",
    "        current_x = x[i-1]\n",
    "        y = sample_from_q(current_x, sigma)\n",
    "\n",
    "        print(f\"\\nStep {i}:\")\n",
    "        print(f\"  Proposto y = {y}\")\n",
    "        print(f\"  Punto attuale x = {current_x}\")\n",
    "\n",
    "        alpha = min(1., p_tilde(y)/ p_tilde(current_x) * q_y_given_x(current_x, y, sigma) / q_y_given_x(y, current_x, sigma))\n",
    "\n",
    "        print(f\"alpha: {alpha} \\n 1^:{ p_tilde(y)/ p_tilde(current_x)} \\n 2^: = {q_y_given_x(current_x, y, sigma) }   { q_y_given_x(y, current_x, sigma)}\")\n",
    "\n",
    "        if np.random.rand()  < alpha:\n",
    "            print(\"  >>> Accettato\")\n",
    "            x[i] = y\n",
    "        else:\n",
    "            print(\"  >>> Rifiutato\")\n",
    "            x[i] = current_x\n",
    "            rejections += 1\n",
    "\n",
    "    return x, rejections\n",
    "\n",
    "def within_variance(m, n, initial_value, sigma):\n",
    "    initial_value = np.asarray(initial_value)  \n",
    "    burn_in = int(n/2)\n",
    "\n",
    "    if len(initial_value) != m:\n",
    "        raise ValueError(f\"{initial_value} Incompatible length\")\n",
    "    \n",
    "    S = []\n",
    "    mean  = []\n",
    "    \n",
    "    for x_0 in initial_value:\n",
    "        x,_ = metropolis_hastings(x_0, n, sigma)\n",
    "        x = x[burn_in:]\n",
    "        mean = np.append(mean, np.mean(x))\n",
    "        s = sum((x - mean[-1]) ** 2) / (n - 1)\n",
    "        S = np.append(S, s)\n",
    "\n",
    "    within_variance = sum(S)/m\n",
    "    between_variance = sum((mean - np.mean(mean)) ** 2) * (n / (m - 1))\n",
    "    upper_buond = ((n - 1) / m) * within_variance + (1 / n) * between_variance\n",
    "    R = np.sqrt(upper_buond / within_variance)\n",
    "\n",
    "    return print(f'within variance: {within_variance} \\n between variance: {between_variance} \\n upper bound: {upper_buond} \\n R: {R}')\n",
    "\n",
    "    \n",
    "\n",
    "t = np.linspace(-10, 10, 1000)\n",
    "plt.plot(t, p_tilde(t), label='p_tilde')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "within_variance(4, 1000, [-10, -2, 2, 10], 2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "burn_in = 500 # number of samples to discard before collecting data\n",
    "n = 1000\n",
    "\n",
    "x_1,r_1 = metropolis_hastings(-10, n, sigma = 0.1)\n",
    "x_2,r_2 = metropolis_hastings(-2, n, sigma = 0.1)\n",
    "x_3,r_3 = metropolis_hastings(2, n, sigma = 0.1)  \n",
    "x_4,r_4 = metropolis_hastings(10, n, sigma = 0.1)\n",
    "\n",
    "y_1,s_1 = metropolis_hastings(-10, n, sigma = 2)\n",
    "y_2,s_2 = metropolis_hastings(-2, n, sigma = 2)\n",
    "y_3,s_3 = metropolis_hastings(2, n, sigma = 2)  \n",
    "y_4,s_4 = metropolis_hastings(10, n, sigma = 2)\n",
    "\n",
    "x_1 = x_1[burn_in:]\n",
    "x_2 = x_2[burn_in:]\n",
    "x_3 = x_3[burn_in:]\n",
    "x_4 = x_4[burn_in:]\n",
    "\n",
    "y_1 = y_1[burn_in:]\n",
    "y_2 = y_2[burn_in:]\n",
    "y_3 = y_3[burn_in:]\n",
    "y_4 = y_4[burn_in:]\n",
    "\n",
    "# print('x:', len(x))\n",
    "# print('Rejection rate:', r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8)) \n",
    "plt.suptitle('Varianza = 0.1', fontsize=16) \n",
    "\n",
    "axs[0, 0].plot(t, p_tilde(t), label='p(x)')\n",
    "axs[0, 0].hist(x_1, bins=50, density=True, alpha=0.7, label='samples')\n",
    "axs[0, 0].set_title('Inizializzazione a x0 = -10')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "axs[0, 1].plot(t, p_tilde(t), label='p(x)')\n",
    "axs[0, 1].hist(x_2, bins=50, density=True, alpha=0.7, label='samples')\n",
    "axs[0, 1].set_title('Inizializzazione a x0 = -2')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "axs[1, 0].plot(t, p_tilde(t), label='p(x)')\n",
    "axs[1, 0].hist(x_3, bins=50, density=True, alpha=0.7, label='samples')\n",
    "axs[1, 0].set_title('Inizializzazione a x0 = 2')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "axs[1, 1].plot(t, p_tilde(t), label='p(x)')\n",
    "axs[1, 1].hist(x_4, bins=50, density=True, alpha=0.7, label='samples')\n",
    "axs[1, 1].set_title('Inizializzazione a x0 = 10')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(0, 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8)) \n",
    "plt.suptitle('Varianza = 2', fontsize=16) \n",
    "\n",
    "axs[0, 0].plot(t, p_tilde(t), label='p(x)')\n",
    "axs[0, 0].hist(y_1, bins=50, density=True, alpha=0.7, label='samples')\n",
    "axs[0, 0].set_title('Inizializzazione a x0 = -10')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "axs[0, 1].plot(t, p_tilde(t), label='p(x)')\n",
    "axs[0, 1].hist(y_2, bins=50, density=True, alpha=0.7, label='samples')\n",
    "axs[0, 1].set_title('Inizializzazione a x0 = -2')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "axs[1, 0].plot(t, p_tilde(t), label='p(x)')\n",
    "axs[1, 0].hist(y_3, bins=50, density=True, alpha=0.7, label='samples')\n",
    "axs[1, 0].set_title('Inizializzazione a x0 = 2')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "axs[1, 1].plot(t, p_tilde(t), label='p(x)')\n",
    "axs[1, 1].hist(y_4, bins=50, density=True, alpha=0.7, label='samples')\n",
    "axs[1, 1].set_title('Inizializzazione a x0 = 10')\n",
    "axs[1, 1].legend()\n",
    "for ax in axs.flat:\n",
    "    ax.set_xlim(-10, 10)\n",
    "    ax.set_ylim(0, 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm.tsa.acf(x, nlags=5)\n",
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "tsaplots.plot_acf(x_1, lags=20, ax=axs[0, 0])\n",
    "axs[0, 0].set_title('ACF x_1 (x0 = -10)')\n",
    "\n",
    "tsaplots.plot_acf(x_2, lags=20, ax=axs[0, 1])\n",
    "axs[0, 1].set_title('ACF x_2 (x0 = -2)')\n",
    "\n",
    "tsaplots.plot_acf(x_3, lags=20, ax=axs[1, 0])\n",
    "axs[1, 0].set_title('ACF x_3 (x0 = 2)')\n",
    "\n",
    "tsaplots.plot_acf(x_4, lags=20, ax=axs[1, 1])\n",
    "axs[1, 1].set_title('ACF x_4 (x0 = 10)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Autocorrelazioni per diverse inizializzazioni ( Varianza = 0.1 )', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "tsaplots.plot_acf(y_1, lags=20, ax=axs[0, 0])\n",
    "axs[0, 0].set_title('ACF y_1 (y0 = -10)')\n",
    "\n",
    "tsaplots.plot_acf(y_2, lags=20, ax=axs[0, 1])\n",
    "axs[0, 1].set_title('ACF y_2 (y0 = -2)')\n",
    "\n",
    "tsaplots.plot_acf(y_3, lags=20, ax=axs[1, 0])\n",
    "axs[1, 0].set_title('ACF y_3 (y0 = 2)')\n",
    "\n",
    "tsaplots.plot_acf(y_4, lags=20, ax=axs[1, 1])\n",
    "axs[1, 1].set_title('ACF y_4 (y0 = 10)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Autocorrelazioni per diverse inizializzazioni ( Varianza = 2 )', fontsize=16)\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: ADSAI football matches\n",
    "Over the years, the PhD students of ADSAI have kept track of the results of their evening five-a-side football matches. Since it’s difficult to always organize teams with the same players, the teams were formed on a rotating basis, each time with different players.  \n",
    "The names of our champions have been anonymized using numbers from 1 to 62.  \n",
    "In the dataset located at `data/ADSAI_football.csv` in the GitHub repository, you will find the following columns:\n",
    "- **Team A**: IDs of the players who played in Team A in that match;  \n",
    "- **Team B**: same as above, for Team B;  \n",
    "- **Goal A**: total goals scored by Team A in that match;  \n",
    "- **Goal B**: same as above, for Team B.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Your goal is to model **the skill of each player** based on information about the team they belonged to and the overall result achieved by that team.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model to implement consists in the following structure: \n",
    "- $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62}) \\in \\mathbb{R} $: players' skills.\n",
    "- $M=54$: number of matches.\n",
    "- For each match $ i = 1, \\dots, M=54$:\n",
    "  - $ A_i \\subset \\{1, \\dots, N=62\\} $: set of players IDs of team A in match $ i $.\n",
    "  - $ B_i \\subset \\{1, \\dots, N=62\\} $: set of players IDs of team B in match $ i $.\n",
    "  - $ y_i \\in \\mathbb{Z} $: observed outcome, i.e. goal difference between the two teams, defined as $ (\\text{goal}_A - \\text{goal}_B) $ in match $ i $.\n",
    "\n",
    "*(in this exercise, you are asked to follow the proposal of Karlis and Ntzoufras approach, that focuses on the goal difference in place of the individual goal counts of each team!)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The model is defined as follows:\n",
    "\n",
    "- The \"strength\" of Team A, defined as the sum of the individual players' skills $\\theta_j$ composing the team in match $i = 1, \\dots, M=54$:  \n",
    "  $$\n",
    "  s_A^{(i)} = \\sum_{j \\in A_i} \\theta_j\n",
    "  $$\n",
    "\n",
    "- The \"strength\" of Team B, defined similarly as the sum of the individual players' skills $\\theta_j$ composing the team in match $i = 1, \\dots, M=54$:  \n",
    "  $$\n",
    "  s_B^{(i)} = \\sum_{j \\in B_i} \\theta_j\n",
    "  $$\n",
    "\n",
    "Specifically, the observed goal difference in match $i$ is modeled using a Skellam distribution:  \n",
    "$$\n",
    "y_i \\sim \\text{Skellam}(\\lambda_A^{(i)}, \\lambda_B^{(i)}), \\quad \\text{where} \\quad \\lambda_A^{(i)} = \\exp(s_A^{(i)}), \\quad \\lambda_B^{(i)} = \\exp(s_B^{(i)})\n",
    "$$\n",
    "\n",
    "The **Skellam distribution** models the difference between two independent random variables:  \n",
    "$$\n",
    "\\text{Skellam}(\\lambda_A, \\lambda_B) = \\text{Poisson}(\\lambda_A) - \\text{Poisson}(\\lambda_B)\n",
    "$$\n",
    "It is formally defined as:  \n",
    "$$\n",
    "\\text{Skellam}(k; \\lambda_A, \\lambda_B) = e^{-(\\lambda_A + \\lambda_B)} \\left( \\frac{\\lambda_A}{\\lambda_B} \\right)^{k/2} I_{|k|}(2 \\sqrt{\\lambda_A \\lambda_B})\n",
    "$$\n",
    "\n",
    "for each $ k \\in \\mathbb{Z} $, and $ I_k $ is the modified Bessel function of the first kind of order $ k $.\n",
    "\n",
    "$$\n",
    "I_k(z) = \\sum_{m=0}^\\infty \\frac{1}{m! \\, \\Gamma(m + k + 1)} \\left( \\frac{z}{2} \\right)^{2m + k}\n",
    "$$\n",
    "\n",
    "where $\\Gamma$ is the Gamma function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import only relevant quantities as follows:\n",
    "\n",
    "import pandas as pd\n",
    "import ast\n",
    "import torch\n",
    "\n",
    "def preprocessing_dataset(dataset_path='../data/ADSAI_football.csv'):\n",
    "    football = pd.read_csv(dataset_path)\n",
    "    football['Team A'] = football['Team A'].apply(ast.literal_eval)\n",
    "    football['Team B'] = football['Team B'].apply(ast.literal_eval)\n",
    "\n",
    "    max_player_id = max(\n",
    "    max(p for team in football['Team A'] for p in team),\n",
    "    max(p for team in football['Team B'] for p in team)\n",
    "    )\n",
    "\n",
    "    goal_diff = torch.tensor((football['Goal A'] - football['Goal B']).values, dtype=torch.int)\n",
    "\n",
    "    teams_A = [torch.tensor(team) for team in football['Team A']]\n",
    "    teams_B = [torch.tensor(team) for team in football['Team B']]\n",
    "\n",
    "    return teams_A, teams_B, goal_diff, max_player_id\n",
    "\n",
    "\n",
    "teams_A, teams_B, goal_diff, max_player_id = preprocessing_dataset()\n",
    "print(\"teams_A:\", teams_A)\n",
    "print(\"teams_B:\", teams_B)\n",
    "print(\"goal_diff:\", goal_diff)\n",
    "print(\"max_player_id:\", max_player_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You are asked to: \n",
    "1. Find the MAP estimate for $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62})$ choosing as `log_prior` function a (log-)Standard Normal distribution and as `log_likelihood` function the (log-)Skellam. Perform optimization using Gradient Descent. Use the MAP estimate to implement a Laplace approximation of the posterior, as done during the lab (reuse the function `compute_hessian(f, w)` of Notebook 6).\n",
    "\n",
    "**Important Hint:** The Skellam log-likelihood involves the modified Bessel function $I_k(z)$, which is non-differentiable in PyTorch if evaluated via scipy. To preserve differentiability, replace $\\log I_k(z)$ with a smooth approximation, such as an asymptotic expansion, to allow gradient-based optimization.\n",
    "\n",
    "2. **(Useful for the next point, not strictly necessary for the previous one):** Implement your `Skellam` distribution, inheriting from `torch.distributions.Distribution`;\n",
    "3. Write the Pyro model corresponding to the problem depicted above assuming (again) the `theta` values being distributed initially as a Standard Normal;\n",
    "4. Perform inference on $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62})$ values running a MCMC simulation using the `NUTS` kernel;\n",
    "5. Compare the `theta` values obtained by these two options using the `performances_evaluation` function given in this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial guess 1: tensor([ 1.1294, -0.8721,  1.9199,  1.5140,  1.6632,  1.3644,  0.0821, -0.5383,\n",
      "         0.3651,  0.6609, -1.1781,  2.0489,  1.4470, -0.6436, -0.6201, -0.7465,\n",
      "         2.1286,  0.6165, -0.3305,  0.2446,  0.7473,  1.1628,  0.5824, -0.3945,\n",
      "         0.4193, -0.3582,  1.1578, -0.9235,  0.1165, -1.0924,  1.7024, -0.4154,\n",
      "         0.7141, -1.4859, -0.2592,  0.5110,  1.7576,  0.8479, -1.3874, -1.2181,\n",
      "         1.3761,  1.0408, -0.3430, -0.6162,  0.7718,  1.3587,  0.5252, -0.9933,\n",
      "        -0.7737, -0.0458,  1.1904,  1.0166,  0.1246, -0.5711, -1.1542, -0.0062,\n",
      "        -0.4220,  2.3039, -1.3311,  0.5554,  0.3187, -0.0488, -0.8406],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tomma\\AppData\\Local\\Temp\\ipykernel_26588\\1581486452.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w = torch.tensor(initial_guess, dtype=torch.float32, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb941cc2f3bb4a828068cf558719a704",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Initial guess 2: tensor([ 1.1294, -0.8721,  1.9199,  1.5140,  1.6632,  1.3644,  0.0821, -0.5383,\n",
      "         0.3651,  0.6609, -1.1781,  2.0489,  1.4470, -0.6436, -0.6201, -0.7465,\n",
      "         2.1286,  0.6165, -0.3305,  0.2446,  0.7473,  1.1628,  0.5824, -0.3945,\n",
      "         0.4193, -0.3582,  1.1578, -0.9235,  0.1165, -1.0924,  1.7024, -0.4154,\n",
      "         0.7141, -1.4859, -0.2592,  0.5110,  1.7576,  0.8479, -1.3874, -1.2181,\n",
      "         1.3761,  1.0408, -0.3430, -0.6162,  0.7718,  1.3587,  0.5252, -0.9933,\n",
      "        -0.7737, -0.0458,  1.1904,  1.0166,  0.1246, -0.5711, -1.1542, -0.0062,\n",
      "        -0.4220,  2.3039, -1.3311,  0.5554,  0.3187, -0.0488, -0.8406],\n",
      "       requires_grad=True)\n",
      "theta: tensor([ 1.1294, -0.8721,  1.9199,  1.5140,  1.6632,  1.3644,  0.0821, -0.5383,\n",
      "         0.3651,  0.6609, -1.1781,  2.0489,  1.4470, -0.6436, -0.6201, -0.7465,\n",
      "         2.1286,  0.6165, -0.3305,  0.2446,  0.7473,  1.1628,  0.5824, -0.3945,\n",
      "         0.4193, -0.3582,  1.1578, -0.9235,  0.1165, -1.0924,  1.7024, -0.4154,\n",
      "         0.7141, -1.4859, -0.2592,  0.5110,  1.7576,  0.8479, -1.3874, -1.2181,\n",
      "         1.3761,  1.0408, -0.3430, -0.6162,  0.7718,  1.3587,  0.5252, -0.9933,\n",
      "        -0.7737, -0.0458,  1.1904,  1.0166,  0.1246, -0.5711, -1.1542, -0.0062,\n",
      "        -0.4220,  2.3039, -1.3311,  0.5554,  0.3187, -0.0488, -0.8406],\n",
      "       requires_grad=True)\n",
      "Initial guess 3: tensor([ 1.1181, -1.2424,  1.8166,  1.3224,  1.5175,  0.9957,     nan, -0.6131,\n",
      "         0.3663,  0.2992,     nan,  0.9145,  1.0775, -0.6388, -0.7086,     nan,\n",
      "            nan,     nan,     nan, -0.1341,  0.4444,     nan,     nan,     nan,\n",
      "            nan, -0.5265,     nan,     nan, -0.2397,     nan,     nan, -0.4401,\n",
      "         0.5990, -1.8261, -0.3657,     nan,     nan,  0.8249, -1.3649, -1.2272,\n",
      "            nan, -4.2714, -0.3305,     nan,  0.6839, -0.7833,  0.4398, -1.6362,\n",
      "        -0.9663, -0.1044,  1.0160,  0.9262,     nan,     nan, -1.1638, -0.1023,\n",
      "        -0.5139,  1.7641, -1.2669,  0.4907,  0.2564, -0.3907, -0.9593],\n",
      "       requires_grad=True)\n",
      "Iteration 2\n",
      "Initial guess 2: tensor([ 1.1181, -1.2424,  1.8166,  1.3224,  1.5175,  0.9957,     nan, -0.6131,\n",
      "         0.3663,  0.2992,     nan,  0.9145,  1.0775, -0.6388, -0.7086,     nan,\n",
      "            nan,     nan,     nan, -0.1341,  0.4444,     nan,     nan,     nan,\n",
      "            nan, -0.5265,     nan,     nan, -0.2397,     nan,     nan, -0.4401,\n",
      "         0.5990, -1.8261, -0.3657,     nan,     nan,  0.8249, -1.3649, -1.2272,\n",
      "            nan, -4.2714, -0.3305,     nan,  0.6839, -0.7833,  0.4398, -1.6362,\n",
      "        -0.9663, -0.1044,  1.0160,  0.9262,     nan,     nan, -1.1638, -0.1023,\n",
      "        -0.5139,  1.7641, -1.2669,  0.4907,  0.2564, -0.3907, -0.9593],\n",
      "       requires_grad=True)\n",
      "theta: tensor([ 1.1181, -1.2424,  1.8166,  1.3224,  1.5175,  0.9957,     nan, -0.6131,\n",
      "         0.3663,  0.2992,     nan,  0.9145,  1.0775, -0.6388, -0.7086,     nan,\n",
      "            nan,     nan,     nan, -0.1341,  0.4444,     nan,     nan,     nan,\n",
      "            nan, -0.5265,     nan,     nan, -0.2397,     nan,     nan, -0.4401,\n",
      "         0.5990, -1.8261, -0.3657,     nan,     nan,  0.8249, -1.3649, -1.2272,\n",
      "            nan, -4.2714, -0.3305,     nan,  0.6839, -0.7833,  0.4398, -1.6362,\n",
      "        -0.9663, -0.1044,  1.0160,  0.9262,     nan,     nan, -1.1638, -0.1023,\n",
      "        -0.5139,  1.7641, -1.2669,  0.4907,  0.2564, -0.3907, -0.9593],\n",
      "       requires_grad=True)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected value argument (Tensor of shape (63,)) to be within the support (Real()) of the distribution Normal(loc: 0.0, scale: 1.0), but found invalid values:\ntensor([ 1.1181, -1.2424,  1.8166,  1.3224,  1.5175,  0.9957,     nan, -0.6131,\n         0.3663,  0.2992,     nan,  0.9145,  1.0775, -0.6388, -0.7086,     nan,\n            nan,     nan,     nan, -0.1341,  0.4444,     nan,     nan,     nan,\n            nan, -0.5265,     nan,     nan, -0.2397,     nan,     nan, -0.4401,\n         0.5990, -1.8261, -0.3657,     nan,     nan,  0.8249, -1.3649, -1.2272,\n            nan, -4.2714, -0.3305,     nan,  0.6839, -0.7833,  0.4398, -1.6362,\n        -0.9663, -0.1044,  1.0160,  0.9262,     nan,     nan, -1.1638, -0.1023,\n        -0.5139,  1.7641, -1.2669,  0.4907,  0.2564, -0.3907, -0.9593],\n       requires_grad=True)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 88\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# def compute_hessian(f, w):\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m#     # TODO\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m#     return ...\u001b[39;00m\n\u001b[32m     85\u001b[39m \n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# here we want to obtain \u001b[39;00m\n\u001b[32m     87\u001b[39m sampled_theta = Normal(loc=\u001b[32m0.0\u001b[39m, scale=\u001b[32m1.0\u001b[39m).sample((max_player_id + \u001b[32m1\u001b[39m,))\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m theta_MAP = \u001b[43mgradient_descent_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_guess\u001b[49m\u001b[43m=\u001b[49m\u001b[43msampled_theta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m theta_MAP\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# posterior_cov = ...\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mgradient_descent_optimization\u001b[39m\u001b[34m(loss_function, lr, n_iter, initial_guess)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInitial guess 2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m loss = \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m loss.backward()\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(theta)\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m log_likelihood(teams_A, teams_B, goal_diff, theta) + log_prior(theta)\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# def loss_function(theta):\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m loss_function = \u001b[38;5;28;01mlambda\u001b[39;00m theta: -\u001b[43mlog_unnormalized_posterior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteams_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteams_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgoal_diff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgradient_descent_optimization\u001b[39m(loss_function, lr: \u001b[38;5;28mfloat\u001b[39m, n_iter: \u001b[38;5;28mint\u001b[39m, initial_guess: torch.Tensor) -> np.ndarray:\n\u001b[32m     68\u001b[39m     w = torch.tensor(initial_guess, dtype=torch.float32, requires_grad=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mlog_unnormalized_posterior\u001b[39m\u001b[34m(teams_A, teams_B, goal_diff, theta)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_unnormalized_posterior\u001b[39m(teams_A, teams_B, goal_diff, theta):\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m log_likelihood(teams_A, teams_B, goal_diff, theta) + \u001b[43mlog_prior\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mlog_prior\u001b[39m\u001b[34m(theta, prior_std)\u001b[39m\n\u001b[32m     57\u001b[39m prior = Normal(loc=\u001b[32m0.0\u001b[39m, scale=prior_std)\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtheta: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprior\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m.sum()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributions\\normal.py:82\u001b[39m, in \u001b[36mNormal.log_prob\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, value):\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._validate_args:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# compute the variance\u001b[39;00m\n\u001b[32m     84\u001b[39m     var = \u001b[38;5;28mself\u001b[39m.scale**\u001b[32m2\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\tomma\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\distributions\\distribution.py:316\u001b[39m, in \u001b[36mDistribution._validate_sample\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    314\u001b[39m valid = support.check(value)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid.all():\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    317\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected value argument \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    318\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    319\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto be within the support (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(support)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof the distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Expected value argument (Tensor of shape (63,)) to be within the support (Real()) of the distribution Normal(loc: 0.0, scale: 1.0), but found invalid values:\ntensor([ 1.1181, -1.2424,  1.8166,  1.3224,  1.5175,  0.9957,     nan, -0.6131,\n         0.3663,  0.2992,     nan,  0.9145,  1.0775, -0.6388, -0.7086,     nan,\n            nan,     nan,     nan, -0.1341,  0.4444,     nan,     nan,     nan,\n            nan, -0.5265,     nan,     nan, -0.2397,     nan,     nan, -0.4401,\n         0.5990, -1.8261, -0.3657,     nan,     nan,  0.8249, -1.3649, -1.2272,\n            nan, -4.2714, -0.3305,     nan,  0.6839, -0.7833,  0.4398, -1.6362,\n        -0.9663, -0.1044,  1.0160,  0.9262,     nan,     nan, -1.1638, -0.1023,\n        -0.5139,  1.7641, -1.2669,  0.4907,  0.2564, -0.3907, -0.9593],\n       requires_grad=True)"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Find the MAP estimate for $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62})$ choosing as `log_prior` function a (log-)Standard Normal distribution and as `log_likelihood` function the (log-)Skellam. Perform optimization using Gradient Descent;\n",
    "\n",
    "from torch.distributions import Normal\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from scipy.special import iv  # Bessel function of the first kind\n",
    "\n",
    "\n",
    "\n",
    "def log_I_alpha_asymptotic(z, alpha):\n",
    "    \"\"\"\n",
    "    Approssimazione del logaritmo della funzione di Bessel modificata del primo tipo I_alpha(z) in PyTorch.\n",
    "    Valida per z → ∞.\n",
    "    \"\"\"\n",
    "    z = z.to(dtype=torch.float32)\n",
    "    alpha = alpha.to(dtype=torch.float32)\n",
    "\n",
    "    sqrt_term = torch.sqrt(1 + (alpha**2) / z**2)\n",
    "    log_prefactor = -0.5 * torch.log(2 * torch.pi * z) - 0.25 * torch.log(1 + (alpha**2) / z**2)\n",
    "    exponent = -alpha * torch.arcsinh(alpha / z) + z * sqrt_term\n",
    "    log_correction = torch.log(1 + 1 / (z * sqrt_term))\n",
    "\n",
    "    return log_prefactor + exponent + log_correction\n",
    "\n",
    "def log_likelihood(teams_A, teams_B, goal_diff, theta):\n",
    "    lambda_A_list = []\n",
    "    lambda_B_list = []\n",
    "\n",
    "\n",
    "    for A_i, B_i in zip(teams_A, teams_B):\n",
    "        s_A = theta[A_i].sum()\n",
    "        s_B = theta[B_i].sum()\n",
    "        lambda_A = torch.exp(s_A)\n",
    "        lambda_B = torch.exp(s_B)\n",
    "    \n",
    "        lambda_A_list.append(lambda_A)\n",
    "        lambda_B_list.append(lambda_B)\n",
    "\n",
    "    # Converti in tensori\n",
    "    lambda_A = torch.stack(lambda_A_list)\n",
    "    lambda_B = torch.stack(lambda_B_list)\n",
    "    \n",
    "    \"\"\"\n",
    "    Approssimazione differenziabile della log-probabilità Skellam.\n",
    "    \"\"\"\n",
    "    z = 2 * torch.sqrt(lambda_A * lambda_B)\n",
    "    goal_diff = goal_diff.to(lambda_A.device)\n",
    "    goal_diff_abs = goal_diff.abs()\n",
    "\n",
    "    log_bessel = log_I_alpha_asymptotic(goal_diff_abs, z)\n",
    "    log_ratio_term = 0.5 * goal_diff * torch.log((lambda_A) / (lambda_B))\n",
    "    log_p = - (lambda_A + lambda_B) + log_ratio_term + log_bessel\n",
    "    return log_p.sum()\n",
    "\n",
    "def log_prior(theta, prior_std=1.0):\n",
    "    prior = Normal(loc=0.0, scale=prior_std)\n",
    "    print(f'theta: {theta}')\n",
    "    return prior.log_prob(theta).sum()\n",
    "\n",
    "def log_unnormalized_posterior(teams_A, teams_B, goal_diff, theta):\n",
    "    return log_likelihood(teams_A, teams_B, goal_diff, theta) + log_prior(theta)\n",
    "\n",
    "# def loss_function(theta):\n",
    "loss_function = lambda theta: -log_unnormalized_posterior(teams_A, teams_B, goal_diff, theta)\n",
    "\n",
    "def gradient_descent_optimization(loss_function, lr: float, n_iter: int, initial_guess: torch.Tensor) -> np.ndarray:\n",
    "    w = torch.tensor(initial_guess, dtype=torch.float32, requires_grad=True)\n",
    "    print(f\"Initial guess 1: {w}\")\n",
    "    for _ in tqdm(range(n_iter)):\n",
    "        print(f\"Iteration {_+1}\")\n",
    "        print(f\"Initial guess 2: {w}\")\n",
    "        loss = loss_function(w)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= lr * w.grad\n",
    "            print(f\"Initial guess 3: {w}\")\n",
    "        w.grad = None\n",
    "    print(f\"loss={ loss.item():.4g}\")\n",
    "    return w.cpu().detach().numpy()\n",
    "\n",
    "def compute_hessian(f, w):\n",
    "    w.requires_grad = True\n",
    "    grad = torch.autograd.grad(f(w), w, create_graph=True)[0]\n",
    "    hessian = torch.zeros((w.numel(), w.numel()))\n",
    "    for i in range(w.numel()):\n",
    "        hessian[i] = torch.autograd.grad(grad[i], w, retain_graph=True)[0]\n",
    "    w.requires_grad = False\n",
    "    return hessian\n",
    "\n",
    "hessian = compute_hessian(f=lambda w : log_unnormalized_posterior(X=X_tensor, y=y_tensor, w=w), w=torch.tensor(w_map, dtype=torch.float32))\n",
    "\n",
    "# here we want to obtain \n",
    "sampled_theta = Normal(loc=0.0, scale=1.0).sample((max_player_id + 1,))\n",
    "theta_MAP = gradient_descent_optimization(loss_function = loss_function, lr=1e-2, n_iter=1000, initial_guess=sampled_theta)\n",
    "posterior_cov = torch.inverse(-hessian).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the marginal distribution of some thetas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "# Sample from the full posterior\n",
    "mvn = MultivariateNormal(loc=theta_MAP, covariance_matrix=torch.from_numpy(posterior_cov).float())\n",
    "posterior_samples = mvn.sample((1000,)) \n",
    "\n",
    "# Indices of thetas you want to visualize\n",
    "selected_indices = [0, 1, 2, 20]  # Change these to the indices you're interested in\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    plt.hist(posterior_samples[:, idx].numpy(), bins=40, density=True, alpha=0.7)\n",
    "    plt.title(f\"Posterior of $\\\\theta_{{{idx}}}$\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Implement your `Skellam` distribution, inheriting from `torch.distributions.Distribution`;\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from torch.distributions import constraints\n",
    "from torch.distributions.distribution import Distribution\n",
    "import scipy.special\n",
    "\n",
    "\n",
    "class Skellam(Distribution):\n",
    "    arg_constraints = {\n",
    "        \"lambdaA\": constraints.positive,\n",
    "        \"lambdaB\": constraints.positive\n",
    "    }\n",
    "    support = constraints.dependent\n",
    "    has_rsample = False\n",
    "\n",
    "    def __init__(self, lambdaA, lambdaB, validate_args=None):\n",
    "        self.lambdaA, self.lambdaB = torch.broadcast_tensors(lambdaA, lambdaB)\n",
    "        batch_shape = self.lambdaA.shape\n",
    "        super().__init__(batch_shape, validate_args=validate_args)\n",
    "\n",
    "    def sample(self, sample_shape=torch.Size()):\n",
    "        shape = self._extended_shape(sample_shape)\n",
    "        A = torch.poisson(self.lambdaA.expand(shape))\n",
    "        B = torch.poisson(self.lambdaB.expand(shape))\n",
    "        return A - B\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        k = value\n",
    "        lamA = self.lambdaA\n",
    "        lamB = self.lambdaB\n",
    "\n",
    "        # Calcolo Bessel con SciPy\n",
    "        iv_numpy = scipy.special.iv(k.abs().cpu().numpy(), 2 * (lamA * lamB).sqrt().cpu().numpy())\n",
    "        iv_term = torch.from_numpy(iv_numpy).to(lamA.device)\n",
    "\n",
    "        iv_term = torch.clamp(iv_term, min=1e-20)\n",
    "        log_iv = torch.log(iv_term)\n",
    "\n",
    "        return -(lamA + lamB) + 0.5 * k * torch.log(lamA / lamB) + log_iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# 3. Write the Pyro model corresponding to the problem depicted above assuming (again) the `theta` values being distributed initially as a Standard Normal;\n",
    "def model(observations: torch.Tensor | None = None, num_matches: int | None = None):\n",
    "    \n",
    "    if observations is not None:\n",
    "        num_matches = len(observations)\n",
    "    \n",
    "    # theta: abilità dei giocatori (Standard Normal prior)\n",
    "    theta = pyro.sample(\"theta\", dist.Normal(0., 1.).expand([max_player_id + 1]).to_event(1))\n",
    "\n",
    "    with pyro.plate(\"matches\", num_matches):\n",
    "        s_A = torch.stack([theta[team].sum() for team in teams_A])\n",
    "        s_B = torch.stack([theta[team].sum() for team in teams_B])\n",
    "\n",
    "        lambda_A = torch.exp(s_A)\n",
    "        lambda_B = torch.exp(s_B)\n",
    "\n",
    "        goal_diff = pyro.sample(\"obs\", Skellam(lambda_A, lambda_B), obs=observations)\n",
    "    \n",
    "    return {'obs': goal_diff}\n",
    "\n",
    "\n",
    "# 2: plot the corresponding graphical model\n",
    "print(goal_diff)\n",
    "\n",
    "# observations = goal_diff\n",
    "# pyro.render_model(model, model_kwargs= {'num_matches': 10}, render_distributions=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_1 = pd.DataFrame(model(num_matches=10))    \n",
    "samples_1\n",
    "samples_2 = pd.DataFrame(model(observations=observations))    \n",
    "samples_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# 4. Perform inference on $ \\theta = (\\theta_1, \\theta_2, \\dots, \\theta_{62})$ values running a MCMC simulation using the `NUTS` kernel;\n",
    "\n",
    "from pyro.infer import MCMC, NUTS\n",
    "kernel = NUTS(model) \n",
    "mcmc = MCMC(kernel, num_samples=1000, warmup_steps=500)\n",
    "mcmc.run(goal_diff, teams_A, teams_B, max_player_id)\n",
    "mcmc.summary()\n",
    "\n",
    "theta_MCMC = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Compare the `theta_*` values obtained by these two options using the `performances_evaluation` function given in this notebook.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def predict_goal_diff_skellam(teamA_ids, teamB_ids, theta, n_sim=10_000, posterior_cov=None):\n",
    "    \"\"\"\n",
    "    Predicts the goal difference (Skellam distribution) between two teams using theta.\n",
    "\n",
    "    Args:\n",
    "        teamA_ids (list): Indices of the players in team A.\n",
    "        teamB_ids (list): Indices of the players in team B.\n",
    "        theta (torch.Tensor): Tensor containing the skill levels of the players.\n",
    "        n_sim (int): Number of simulations to run.\n",
    "\n",
    "    Returns:\n",
    "        float: Mean predicted goal difference (A - B).\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the total strength for each team (sum of player skill levels)\n",
    "    sA = theta[teamA_ids].sum()\n",
    "    sB = theta[teamB_ids].sum()\n",
    "\n",
    "    # Lambda parameters for Poisson distribution (expected goals)\n",
    "    lam_A = torch.exp(sA).item()  # Team A's expected goal rate\n",
    "    lam_B = torch.exp(sB).item()  # Team B's expected goal rate\n",
    "\n",
    "    # Simulate goals for each team using Poisson distribution\n",
    "    goals_A = np.random.poisson(lam_A, size=n_sim)\n",
    "    goals_B = np.random.poisson(lam_B, size=n_sim)\n",
    "\n",
    "    # Calculate the difference in goals (A - B)\n",
    "    diff = goals_A - goals_B\n",
    "\n",
    "    # Return the mean predicted difference\n",
    "    return diff.mean()\n",
    "\n",
    "\n",
    "def predict_goal_diff_laplace(teamA_ids, teamB_ids, theta_map, cov_matrix, n_sim=1000):\n",
    "    # Combine all relevant indices\n",
    "    all_ids = torch.tensor(sorted(set(teamA_ids) | set(teamB_ids)))\n",
    "\n",
    "    # Extract subvector of means (mu_A and mu_B)\n",
    "    theta_sub = theta_map[all_ids].float()\n",
    "\n",
    "    # Extract submatrix of covariances\n",
    "    cov_sub = cov_matrix[np.ix_(all_ids, all_ids)]  # numpy version\n",
    "    cov_sub = torch.from_numpy(cov_sub).float()\n",
    "\n",
    "    # Create multivariate normal from posterior\n",
    "    mvn = MultivariateNormal(loc=theta_sub, covariance_matrix=cov_sub)\n",
    "\n",
    "    # Sample from posterior\n",
    "    theta_samples = mvn.sample((n_sim,))  # shape: (n_sim, len(all_ids))\n",
    "\n",
    "    # Compute strength for each team in each sample\n",
    "    teamA_len = len(teamA_ids)\n",
    "    sA = theta_samples[:, :teamA_len].sum(dim=1)\n",
    "    sB = theta_samples[:, teamA_len:].sum(dim=1)\n",
    "\n",
    "    # Expected goals via Poisson lambdas\n",
    "    lam_A = torch.exp(sA)\n",
    "    lam_B = torch.exp(sB)\n",
    "\n",
    "    # Sample goals\n",
    "    goals_A = torch.poisson(lam_A)\n",
    "    goals_B = torch.poisson(lam_B)\n",
    "\n",
    "    # Goal differences\n",
    "    diff = goals_A - goals_B\n",
    "    return diff.mean()\n",
    "\n",
    "\n",
    "def evaluate_performance(theta_MAP, theta_MCMC, posterior_cov, teams_A, teams_B, goal_diff, n_sim=10_000):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of different sets of theta values by comparing the Mean Bias Error (MBE)\n",
    "    and Mean Absolute Error (MAE) between predicted and actual goal differences.\n",
    "\n",
    "    Args:\n",
    "        theta_values (list of torch.Tensor): List of different theta values to evaluate.\n",
    "        teams_A (list of lists): List of player IDs for team A in each match.\n",
    "        teams_B (list of lists): List of player IDs for team B in each match.\n",
    "        goal_diff (list): List of actual goal differences (A - B).\n",
    "        n_sim (int): Number of simulations to run for each set of theta values.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with MBE and MAE for each set of theta values.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Loop through each set of theta values\n",
    " \n",
    "    predicted_diffs_MAP = []  # List to store predicted goal differences\n",
    "    predicted_diffs_MCMC = []\n",
    "    actual_diffs = goal_diff  # Actual goal differences\n",
    "\n",
    "    # Simulate the match results for each game\n",
    "    for i in range(len(goal_diff)):\n",
    "        teamA_ids = teams_A[i]\n",
    "        teamB_ids = teams_B[i]\n",
    "\n",
    "        # Predict the goal difference \n",
    "        # Use the Laplace approximation if posterior_cov is provided\n",
    "        \n",
    "        predicted_MAP = predict_goal_diff_laplace(teamA_ids, teamB_ids, theta_MAP, posterior_cov, n_sim)\n",
    "        predicted_MCMC = predict_goal_diff_skellam(teamA_ids, teamB_ids, theta_MCMC, n_sim)\n",
    "        \n",
    "        predicted_diffs_MAP.append(predicted_MAP)\n",
    "        predicted_diffs_MCMC.append(predicted_MCMC)\n",
    "\n",
    "    # Convert lists to numpy arrays for easier calculations\n",
    "    predicted_diffs_MAP = np.array(predicted_diffs_MAP)\n",
    "    predicted_diffs_MCMC = np.array(predicted_diffs_MCMC)\n",
    "    actual_diffs = np.array(actual_diffs)\n",
    "\n",
    "    # Compute MAE and MBE\n",
    "    mae_MAP = np.mean(np.abs(predicted_diffs_MAP - actual_diffs))  # Mean Absolute Error\n",
    "    mbe_MAP = (predicted_diffs_MAP - actual_diffs).mean()  # Mean Bias Error\n",
    "    mae_MCMC = np.mean(np.abs(predicted_diffs_MCMC - actual_diffs))  # Mean Absolute Error\n",
    "    mbe_MCMC = (predicted_diffs_MCMC - actual_diffs).mean()  # Mean Bias Error\n",
    "\n",
    "    # Store results for the current theta set\n",
    "    results = {'MAE_MAP': mae_MAP, 'MBE_MAP': mbe_MAP,\n",
    "              'MAE_MCMC': mae_MCMC, 'MBE_MCMC': mbe_MCMC}\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Evaluate the performance of different theta values on the same dataset used to fix those values\n",
    "performance_results = evaluate_performance(theta_MCMC, theta_MAP, posterior_cov, teams_A, teams_B, goal_diff)\n",
    "\n",
    "# Print the results\n",
    "print(\"Performance Results:\")\n",
    "print(f\"MAP - MAE: {performance_results['MAE_MAP']}, MBE: {performance_results['MBE_MAP']}\")\n",
    "print(f\"MCMC - MAE: {performance_results['MAE_MCMC']}, MBE: {performance_results['MBE_MCMC']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
